{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:27.305584Z",
     "start_time": "2025-08-04T12:21:24.130546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Phase 1: Data Exploration & Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load OULAD data from CSV files\n",
    "# Using raw strings to handle backslashes in Windows file paths\n",
    "# This function assumes the CSV files are located in the specified directory\n",
    "# and that the directory structure matches the expected format.\n",
    "\n",
    "def load_oulad_data(data_path):\n",
    "    \"\"\"Load all OULAD CSV files into pandas DataFrames\"\"\"\n",
    "\n",
    "    # Core tables - using raw strings\n",
    "    courses = pd.read_csv(rf\"{data_path}\\courses.csv\")\n",
    "    assessments = pd.read_csv(rf\"{data_path}\\assessments.csv\")\n",
    "    vle = pd.read_csv(rf\"{data_path}\\vle.csv\")\n",
    "\n",
    "    # Student data\n",
    "    student_info = pd.read_csv(rf\"{data_path}\\studentInfo.csv\")\n",
    "    student_registration = pd.read_csv(rf\"{data_path}\\studentRegistration.csv\")\n",
    "    student_assessment = pd.read_csv(rf\"{data_path}\\studentAssessment.csv\")\n",
    "    student_vle = pd.read_csv(rf\"{data_path}\\studentVle.csv\")\n",
    "\n",
    "    return {\n",
    "        'courses': courses,\n",
    "        'assessments': assessments,\n",
    "        'vle': vle,\n",
    "        'student_info': student_info,\n",
    "        'student_registration': student_registration,\n",
    "        'student_assessment': student_assessment,\n",
    "        'student_vle': student_vle\n",
    "    }\n",
    "\n",
    "# Load the data - use raw string for path\n",
    "oulad_data = load_oulad_data(r\"C:\\Users\\Ritam\\Projects\\XAIDashboard\\dataset\")"
   ],
   "id": "6181ece2c20bf6d7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:33.816683Z",
     "start_time": "2025-08-04T12:21:33.801221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#data exploration\n",
    "# Function to explore the main student information table\n",
    "def explore_student_data(oulad_data):\n",
    "    \"\"\"Explore the main student information table\"\"\"\n",
    "\n",
    "    student_info = oulad_data['student_info']\n",
    "\n",
    "    print(\"=== STUDENT INFO EXPLORATION ===\")\n",
    "    print(f\"Shape: {student_info.shape}\")\n",
    "    print(f\"Columns: {list(student_info.columns)}\")\n",
    "    print(\"\\n--- Sample Data ---\")\n",
    "    print(student_info.head(3))\n",
    "\n",
    "    print(\"\\n--- Target Variable Distribution ---\")\n",
    "    print(student_info['final_result'].value_counts())\n",
    "    print(student_info['final_result'].value_counts(normalize=True).round(3))\n",
    "\n",
    "    print(\"\\n--- Demographic Breakdown ---\")\n",
    "    print(\"Gender:\", student_info['gender'].value_counts().to_dict())\n",
    "    print(\"Age bands:\", student_info['age_band'].value_counts().to_dict())\n",
    "    print(\"Disability:\", student_info['disability'].value_counts().to_dict())\n",
    "\n",
    "# Run exploration\n",
    "explore_student_data(oulad_data)"
   ],
   "id": "576f3cdcd4cfa1c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STUDENT INFO EXPLORATION ===\n",
      "Shape: (32593, 12)\n",
      "Columns: ['code_module', 'code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'final_result']\n",
      "\n",
      "--- Sample Data ---\n",
      "  code_module code_presentation  id_student gender                region  \\\n",
      "0         AAA             2013J       11391      M   East Anglian Region   \n",
      "1         AAA             2013J       28400      F              Scotland   \n",
      "2         AAA             2013J       30268      F  North Western Region   \n",
      "\n",
      "       highest_education imd_band age_band  num_of_prev_attempts  \\\n",
      "0       HE Qualification  90-100%     55<=                     0   \n",
      "1       HE Qualification   20-30%    35-55                     0   \n",
      "2  A Level or Equivalent   30-40%    35-55                     0   \n",
      "\n",
      "   studied_credits disability final_result  \n",
      "0              240          N         Pass  \n",
      "1               60          N         Pass  \n",
      "2               60          Y    Withdrawn  \n",
      "\n",
      "--- Target Variable Distribution ---\n",
      "final_result\n",
      "Pass           12361\n",
      "Withdrawn      10156\n",
      "Fail            7052\n",
      "Distinction     3024\n",
      "Name: count, dtype: int64\n",
      "final_result\n",
      "Pass           0.379\n",
      "Withdrawn      0.312\n",
      "Fail           0.216\n",
      "Distinction    0.093\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Demographic Breakdown ---\n",
      "Gender: {'M': 17875, 'F': 14718}\n",
      "Age bands: {'0-35': 22944, '35-55': 9433, '55<=': 216}\n",
      "Disability: {'N': 29429, 'Y': 3164}\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:40.324699Z",
     "start_time": "2025-08-04T12:21:40.216389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Check VLE Engagement Patterns\n",
    "def explore_vle_data(oulad_data):\n",
    "    \"\"\"Explore VLE interaction patterns\"\"\"\n",
    "\n",
    "    student_vle = oulad_data['student_vle']\n",
    "\n",
    "    print(\"=== VLE INTERACTION EXPLORATION ===\")\n",
    "    print(f\"Total interactions: {len(student_vle):,}\")\n",
    "    print(f\"Unique students with VLE data: {student_vle['id_student'].nunique():,}\")\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Clicks per interaction - Mean: {student_vle['sum_click'].mean():.1f}, Max: {student_vle['sum_click'].max()}\")\n",
    "\n",
    "    # Activity patterns\n",
    "    activity_types = oulad_data['vle']['activity_type'].value_counts().head(10)\n",
    "    print(\"\\n--- Top Activity Types ---\")\n",
    "    print(activity_types)\n",
    "\n",
    "# Run VLE exploration\n",
    "explore_vle_data(oulad_data)\n",
    "\n"
   ],
   "id": "9ba91e8baabe9c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VLE INTERACTION EXPLORATION ===\n",
      "Total interactions: 10,655,280\n",
      "Unique students with VLE data: 26,074\n",
      "Clicks per interaction - Mean: 3.7, Max: 6977\n",
      "\n",
      "--- Top Activity Types ---\n",
      "activity_type\n",
      "resource         2660\n",
      "subpage          1055\n",
      "oucontent         996\n",
      "url               886\n",
      "forumng           194\n",
      "quiz              127\n",
      "page              102\n",
      "oucollaborate      82\n",
      "questionnaire      61\n",
      "ouwiki             49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:42.841444Z",
     "start_time": "2025-08-04T12:21:42.142480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Assess Data Quality\n",
    "def assess_data_quality(oulad_data):\n",
    "    \"\"\"Quick data quality assessment\"\"\"\n",
    "\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "    for table_name, df in oulad_data.items():\n",
    "        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "        print(f\"{table_name}: {missing_pct:.1f}% missing values\")\n",
    "\n",
    "        if missing_pct > 0:\n",
    "            missing_cols = df.isnull().sum()\n",
    "            critical_missing = missing_cols[missing_cols > len(df) * 0.1]  # >10% missing\n",
    "            if len(critical_missing) > 0:\n",
    "                print(f\"  âš ï¸  High missingness in: {critical_missing.to_dict()}\")\n",
    "\n",
    "# Run quality assessment\n",
    "assess_data_quality(oulad_data)\n"
   ],
   "id": "542f4321f752e7de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY ASSESSMENT ===\n",
      "courses: 0.0% missing values\n",
      "assessments: 0.9% missing values\n",
      "vle: 27.5% missing values\n",
      "  âš ï¸  High missingness in: {'week_from': 5243, 'week_to': 5243}\n",
      "student_info: 0.3% missing values\n",
      "student_registration: 13.8% missing values\n",
      "  âš ï¸  High missingness in: {'date_unregistration': 22521}\n",
      "student_assessment: 0.0% missing values\n",
      "student_vle: 0.0% missing values\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:45.666079Z",
     "start_time": "2025-08-04T12:21:45.660552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VLE Table: week_from and week_to (27.5% missing)\n",
    "def handle_vle_missing(vle_data):\n",
    "    \"\"\"Handle missing week data in VLE table\"\"\"\n",
    "\n",
    "    # Strategy 1: Create 'continuous' category for missing weeks\n",
    "    vle_data['week_from'] = vle_data['week_from'].fillna(-1)  # -1 = continuous\n",
    "    vle_data['week_to'] = vle_data['week_to'].fillna(-1)\n",
    "\n",
    "    # Strategy 2: Create boolean flag for continuous activities\n",
    "    vle_data['is_continuous_activity'] = vle_data['week_from'].isna()\n",
    "\n",
    "    # Strategy 3: Fill with course-wide availability (0 to max_week)\n",
    "    max_week = vle_data['week_to'].max()\n",
    "    vle_data['week_from_filled'] = vle_data['week_from'].fillna(0)\n",
    "    vle_data['week_to_filled'] = vle_data['week_to'].fillna(max_week)\n",
    "\n",
    "    return vle_data\n",
    "\n",
    "#Student Registration: date_unregistration (13.8% missing)\n",
    "def handle_registration_missing(registration_data):\n",
    "    \"\"\"Handle missing unregistration dates\"\"\"\n",
    "\n",
    "    # Strategy 1: Create binary completion indicator\n",
    "    registration_data['completed_course'] = registration_data['date_unregistration'].isna()\n",
    "\n",
    "    # Strategy 2: Fill with end-of-course date for completed students\n",
    "    # (You can get course end dates from course table)\n",
    "    registration_data['unregistration_status'] = registration_data['date_unregistration'].apply(\n",
    "        lambda x: 'completed' if pd.isna(x) else 'withdrew'\n",
    "    )\n",
    "\n",
    "    # Strategy 3: Calculate retention days (for non-missing values)\n",
    "    registration_data['retention_days'] = (\n",
    "        registration_data['date_unregistration'] - registration_data['date_registration']\n",
    "    ).dt.days\n",
    "\n",
    "    return registration_data\n"
   ],
   "id": "147cd076af061ccc",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:49.760470Z",
     "start_time": "2025-08-04T12:21:49.243974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Handle all missing values comprehensively\n",
    "def comprehensive_missing_value_handling(oulad_data):\n",
    "    \"\"\"Complete strategy for handling all missing values\"\"\"\n",
    "\n",
    "    print(\"=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "    # 1. VLE table - handle week missingness\n",
    "    print(\"Processing VLE missing values...\")\n",
    "    vle = oulad_data['vle'].copy()\n",
    "\n",
    "    # Create flags for continuous activities\n",
    "    vle['is_continuous'] = vle['week_from'].isna()\n",
    "\n",
    "    # Fill with meaningful values\n",
    "    vle['week_from'] = vle['week_from'].fillna(0)  # Start of course\n",
    "    vle['week_to'] = vle['week_to'].fillna(vle['week_to'].max())  # End of course\n",
    "\n",
    "    oulad_data['vle'] = vle\n",
    "    print(f\"âœ… VLE: Added continuous activity flags\")\n",
    "\n",
    "    # 2. Student registration - handle unregistration missingness\n",
    "    print(\"Processing registration missing values...\")\n",
    "    registration = oulad_data['student_registration'].copy()\n",
    "\n",
    "    # Create completion indicators\n",
    "    registration['completed_course'] = registration['date_unregistration'].isna()\n",
    "    registration['withdrawal_status'] = registration['date_unregistration'].apply(\n",
    "        lambda x: 'completed' if pd.isna(x) else 'withdrew_early'\n",
    "    )\n",
    "\n",
    "    oulad_data['student_registration'] = registration\n",
    "    print(f\"âœ… Registration: Added completion indicators\")\n",
    "\n",
    "    # 3. Handle minor missing values in other tables\n",
    "    print(\"Processing minor missing values...\")\n",
    "\n",
    "    # Student info - forward fill demographic data (assumed stable)\n",
    "    student_info = oulad_data['student_info'].copy()\n",
    "    demographic_cols = ['gender', 'region', 'highest_education', 'disability']\n",
    "    for col in demographic_cols:\n",
    "        if col in student_info.columns and student_info[col].isna().any():\n",
    "            # Fill with mode (most common value)\n",
    "            mode_value = student_info[col].mode()[0]\n",
    "            student_info[col] = student_info[col].fillna(mode_value)\n",
    "            print(f\"   Filled {col} with mode: {mode_value}\")\n",
    "\n",
    "    oulad_data['student_info'] = student_info\n",
    "\n",
    "    # 4. Verify no critical missing values remain\n",
    "    print(\"\\n=== POST-PROCESSING VERIFICATION ===\")\n",
    "    for table_name, df in oulad_data.items():\n",
    "        critical_missing = df.isnull().sum().sum()\n",
    "        if critical_missing > 0:\n",
    "            print(f\"âš ï¸  {table_name}: {critical_missing} missing values remain\")\n",
    "            missing_cols = df.isnull().sum()\n",
    "            print(f\"   Columns: {missing_cols[missing_cols > 0].to_dict()}\")\n",
    "        else:\n",
    "            print(f\"âœ… {table_name}: No missing values\")\n",
    "\n",
    "    return oulad_data\n",
    "\n",
    "# Apply the comprehensive handling\n",
    "oulad_data_clean = comprehensive_missing_value_handling(oulad_data)\n"
   ],
   "id": "95a8011fd2a90a0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDLING MISSING VALUES ===\n",
      "Processing VLE missing values...\n",
      "âœ… VLE: Added continuous activity flags\n",
      "Processing registration missing values...\n",
      "âœ… Registration: Added completion indicators\n",
      "Processing minor missing values...\n",
      "\n",
      "=== POST-PROCESSING VERIFICATION ===\n",
      "âœ… courses: No missing values\n",
      "âš ï¸  assessments: 11 missing values remain\n",
      "   Columns: {'date': 11}\n",
      "âœ… vle: No missing values\n",
      "âš ï¸  student_info: 1111 missing values remain\n",
      "   Columns: {'imd_band': 1111}\n",
      "âš ï¸  student_registration: 22566 missing values remain\n",
      "   Columns: {'date_registration': 45, 'date_unregistration': 22521}\n",
      "âš ï¸  student_assessment: 173 missing values remain\n",
      "   Columns: {'score': 173}\n",
      "âœ… student_vle: No missing values\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:21:58.176861Z",
     "start_time": "2025-08-04T12:21:57.189099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#4. Feature Engineering from Missing Value Patterns\n",
    "def create_missing_value_features(oulad_data_clean):\n",
    "    \"\"\"Create predictive features from missing value patterns\"\"\"\n",
    "\n",
    "    # From VLE data\n",
    "    vle_features = oulad_data_clean['vle'].groupby(['code_module', 'code_presentation']).agg({\n",
    "        'is_continuous': 'sum',  # Count of continuous activities per course\n",
    "    }).reset_index()\n",
    "    vle_features.rename(columns={'is_continuous': 'continuous_activities_count'}, inplace=True)\n",
    "\n",
    "    # From registration data\n",
    "    registration_features = oulad_data_clean['student_registration'][\n",
    "        ['code_module', 'code_presentation', 'id_student', 'completed_course', 'withdrawal_status']\n",
    "    ]\n",
    "\n",
    "    print(\"âœ… Created predictive features from missing value patterns\")\n",
    "\n",
    "    return vle_features, registration_features\n",
    "\n",
    "def verify_missing_handling(original_data, clean_data):\n",
    "    \"\"\"Verify missing value handling worked correctly\"\"\"\n",
    "\n",
    "    print(\"=== MISSING VALUE HANDLING VERIFICATION ===\")\n",
    "\n",
    "    for table_name in original_data.keys():\n",
    "        original_missing = original_data[table_name].isnull().sum().sum()\n",
    "        clean_missing = clean_data[table_name].isnull().sum().sum()\n",
    "\n",
    "        print(f\"{table_name}:\")\n",
    "        print(f\"  Before: {original_missing} missing values\")\n",
    "        print(f\"  After:  {clean_missing} missing values\")\n",
    "        print(f\"  Reduction: {original_missing - clean_missing}\")\n",
    "\n",
    "        # Check if any new features were created\n",
    "        original_cols = set(original_data[table_name].columns)\n",
    "        clean_cols = set(clean_data[table_name].columns)\n",
    "        new_features = clean_cols - original_cols\n",
    "        if new_features:\n",
    "            print(f\"  âœ… New features: {new_features}\")\n",
    "        print()\n",
    "\n",
    "# Run verification\n",
    "verify_missing_handling(oulad_data, oulad_data_clean)\n",
    "\n"
   ],
   "id": "76006b8d8d8d564e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING VALUE HANDLING VERIFICATION ===\n",
      "courses:\n",
      "  Before: 0 missing values\n",
      "  After:  0 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "assessments:\n",
      "  Before: 11 missing values\n",
      "  After:  11 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "vle:\n",
      "  Before: 0 missing values\n",
      "  After:  0 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "student_info:\n",
      "  Before: 1111 missing values\n",
      "  After:  1111 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "student_registration:\n",
      "  Before: 22566 missing values\n",
      "  After:  22566 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "student_assessment:\n",
      "  Before: 173 missing values\n",
      "  After:  173 missing values\n",
      "  Reduction: 0\n",
      "\n",
      "student_vle:\n",
      "  Before: 0 missing values\n",
      "  After:  0 missing values\n",
      "  Reduction: 0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:22:17.556889Z",
     "start_time": "2025-08-04T12:22:16.034425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Phase 2: Predictive Modeling\n",
    "#Phase 2A: Data Integration & Feature Engineering\n",
    "import numpy as np\n",
    "#Step 1:Combined Dataset\n",
    "def create_combined_dataset(oulad_data_clean):\n",
    "    \"\"\"Combine all OULAD tables into a single analysis-ready dataset\"\"\"\n",
    "\n",
    "    print(\"=== CREATING COMBINED DATASET ===\")\n",
    "\n",
    "    # Start with student_info as base table\n",
    "    combined = oulad_data_clean['student_info'].copy()\n",
    "\n",
    "    # Add registration data with our new completion features\n",
    "    registration = oulad_data_clean['student_registration'][\n",
    "        ['code_module', 'code_presentation', 'id_student', 'completed_course', 'withdrawal_status']\n",
    "    ]\n",
    "    combined = combined.merge(registration, on=['code_module', 'code_presentation', 'id_student'], how='inner')\n",
    "\n",
    "    # Aggregate VLE engagement features\n",
    "    vle_features = create_vle_features_safe(oulad_data_clean['student_vle'])\n",
    "    combined = combined.merge(vle_features, on='id_student', how='left')\n",
    "\n",
    "    # Aggregate assessment performance features\n",
    "    assessment_features = create_assessment_features(oulad_data_clean['student_assessment'])\n",
    "    combined = combined.merge(assessment_features, on='id_student', how='left')\n",
    "\n",
    "    return combined\n",
    "\n",
    "# Implement this first\n",
    "combined_dataset = create_combined_dataset(oulad_data_clean)\n"
   ],
   "id": "c5dac693df26ef79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING COMBINED DATASET ===\n",
      "ğŸ”„ Creating VLE features with safe calculations...\n",
      "âœ… VLE features created for 26074 students\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:28:31.214033Z",
     "start_time": "2025-08-04T12:28:31.207047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fix_infinite_and_extreme_values(df, features):\n",
    "    \"\"\"Clean infinite and extreme values from dataset\"\"\"\n",
    "\n",
    "    print(\"ğŸ§¹ CLEANING INFINITE AND EXTREME VALUES\")\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    issues_found = {}\n",
    "\n",
    "    for col in features:\n",
    "        if col in df_clean.columns:\n",
    "            # Check for issues\n",
    "            infinite_count = np.isinf(df_clean[col]).sum()\n",
    "            extreme_count = (df_clean[col].abs() > 1e10).sum()\n",
    "\n",
    "            if infinite_count > 0 or extreme_count > 0:\n",
    "                issues_found[col] = {'infinite': infinite_count, 'extreme': extreme_count}\n",
    "\n",
    "                # Replace infinite values with NaN\n",
    "                df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "                # Cap extreme values at 99.9th percentile\n",
    "                if extreme_count > 0:\n",
    "                    upper_cap = df_clean[col].quantile(0.999)\n",
    "                    lower_cap = df_clean[col].quantile(0.001)\n",
    "                    df_clean[col] = df_clean[col].clip(lower=lower_cap, upper=upper_cap)\n",
    "\n",
    "                print(f\"   âœ… Fixed {col}: {infinite_count} infinite, {extreme_count} extreme values\")\n",
    "\n",
    "    # Fill remaining missing values with median\n",
    "    for col in features:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"   âœ… Filled {col} missing values with median: {median_val:.2f}\")\n",
    "\n",
    "    print(f\"âœ… Data cleaning complete. Issues found in: {list(issues_found.keys())}\")\n",
    "    return df_clean, issues_found\n"
   ],
   "id": "ce8d68a3fbd7fe6b",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:28:34.121346Z",
     "start_time": "2025-08-04T12:28:34.115293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 2: VLE Behavioral Feature Engineering\n",
    "def create_vle_features_safe(student_vle):\n",
    "    \"\"\"Create VLE engagement features with safe division\"\"\"\n",
    "\n",
    "    print(\"ğŸ”„ Creating VLE features with safe calculations...\")\n",
    "\n",
    "    vle_features = student_vle.groupby('id_student').agg({\n",
    "        'sum_click': ['sum', 'mean', 'std', 'count'],\n",
    "        'date': ['min', 'max', 'nunique']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    vle_features.columns = ['id_student', 'total_clicks', 'avg_clicks_per_session',\n",
    "                           'click_variability', 'total_sessions', 'first_access_day',\n",
    "                           'last_access_day', 'active_days']\n",
    "\n",
    "    # Create derived features with safe division\n",
    "    vle_features['engagement_duration'] = (\n",
    "        vle_features['last_access_day'] - vle_features['first_access_day']\n",
    "    )\n",
    "\n",
    "    # Safe division for daily engagement rate\n",
    "    vle_features['daily_engagement_rate'] = np.where(\n",
    "        vle_features['engagement_duration'] > 0,\n",
    "        vle_features['active_days'] / vle_features['engagement_duration'],\n",
    "        0  # Default to 0 if duration is 0\n",
    "    )\n",
    "\n",
    "    # Cap engagement rate at reasonable maximum (e.g., 1.0)\n",
    "    vle_features['daily_engagement_rate'] = vle_features['daily_engagement_rate'].clip(0, 1.0)\n",
    "\n",
    "    # Fill NaN values from std calculation\n",
    "    vle_features['click_variability'] = vle_features['click_variability'].fillna(0)\n",
    "\n",
    "    print(f\"âœ… VLE features created for {len(vle_features)} students\")\n",
    "    return vle_features\n",
    "\n"
   ],
   "id": "ba7f9e911d9b1c03",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:28:39.430867Z",
     "start_time": "2025-08-04T12:28:39.425868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 3: Assessment Performance Features\n",
    "def create_assessment_features(student_assessment):\n",
    "    \"\"\"Create assessment performance and timing features\"\"\"\n",
    "\n",
    "    assessment_features = student_assessment.groupby('id_student').agg({\n",
    "        'score': ['mean', 'std', 'count'],\n",
    "        'date_submitted': ['min', 'max'],\n",
    "        'is_banked': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten columns\n",
    "    assessment_features.columns = ['id_student', 'avg_assessment_score', 'score_consistency',\n",
    "                                  'total_assessments', 'first_submission', 'last_submission',\n",
    "                                  'banked_assessments']\n",
    "\n",
    "    return assessment_features\n"
   ],
   "id": "99f167e5f56d6427",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:28:43.503773Z",
     "start_time": "2025-08-04T12:28:43.490772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Phase 2B: Model Training Pipeline (This Week)\n",
    "#Step 4: Feature Selection & Preparation\n",
    "\n",
    "def prepare_modeling_data(combined_dataset):\n",
    "    \"\"\"Prepare data for machine learning\"\"\"\n",
    "\n",
    "    # Define feature categories\n",
    "    demographic_features = ['gender', 'age_band', 'highest_education', 'disability', 'region']\n",
    "    behavioral_features = ['total_clicks', 'avg_clicks_per_session', 'active_days', 'daily_engagement_rate']\n",
    "    academic_features = ['avg_assessment_score', 'total_assessments', 'studied_credits']\n",
    "    engagement_features = ['completed_course', 'total_sessions', 'engagement_duration']\n",
    "\n",
    "    # Create feature sets for comparison\n",
    "    all_features = demographic_features + behavioral_features + academic_features + engagement_features\n",
    "    behavior_only = demographic_features + behavioral_features + engagement_features  # No grades\n",
    "\n",
    "    # Handle categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "    # Encode categorical variables\n",
    "    le_dict = {}\n",
    "    processed_data = combined_dataset.copy()\n",
    "\n",
    "    for col in demographic_features:\n",
    "        if processed_data[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            processed_data[col] = le.fit_transform(processed_data[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "\n",
    "    # Handle target variable\n",
    "    target_encoder = LabelEncoder()\n",
    "    processed_data['target'] = target_encoder.fit_transform(processed_data['final_result'])\n",
    "\n",
    "    return processed_data, all_features, behavior_only, target_encoder, le_dict\n",
    "\n",
    "#Step 5: Train-Test Split & Baseline Models\n",
    "def train_baseline_models_safe(processed_data, feature_sets, target_col='target'):\n",
    "    \"\"\"Train models with comprehensive data validation\"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    print(\"ğŸš€ STARTING SAFE BASELINE MODEL TRAINING\")\n",
    "    print(f\"Dataset shape: {processed_data.shape}\")\n",
    "    print(f\"Target distribution:\\n{processed_data[target_col].value_counts()}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for feature_set_name, features in feature_sets.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ¯ TRAINING ON {feature_set_name.upper()} FEATURE SET\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Prepare data with validation\n",
    "        X = processed_data[features].copy()\n",
    "        y = processed_data[target_col].copy()\n",
    "\n",
    "        print(f\"âœ… Initial data: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "        # CRITICAL: Clean infinite and extreme values\n",
    "        X_clean, issues = fix_infinite_and_extreme_values(X, features)\n",
    "\n",
    "        # Final validation\n",
    "        infinite_check = np.isinf(X_clean.select_dtypes(include=[np.number])).any().any()\n",
    "        missing_check = X_clean.isnull().any().any()\n",
    "\n",
    "        if infinite_check:\n",
    "            print(\"âŒ Still have infinite values after cleaning!\")\n",
    "            return None\n",
    "\n",
    "        if missing_check:\n",
    "            print(\"âš ï¸  Still have missing values - final cleanup...\")\n",
    "            X_clean = X_clean.fillna(X_clean.median())\n",
    "\n",
    "        print(f\"âœ… Data validated: No infinite values, No missing values\")\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_clean, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Train/Test split: Train={X_train.shape}, Test={X_test.shape}\")\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            print(\"âœ… Feature scaling successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Scaling failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Train models\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        }\n",
    "\n",
    "        feature_results = {}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nğŸ”„ Training {model_name}...\")\n",
    "\n",
    "            try:\n",
    "                # Train model\n",
    "                if model_name == 'Logistic Regression':\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                print(f\"âœ… {model_name} Results:\")\n",
    "                print(f\"   ğŸ“Š Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"   ğŸ“Š F1-Score (weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "                # Store results\n",
    "                feature_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_score': f1_weighted,\n",
    "                    'scaler': scaler if model_name == 'Logistic Regression' else None,\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {model_name} training failed: {e}\")\n",
    "                continue\n",
    "\n",
    "        results[feature_set_name] = feature_results\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\nğŸ“ˆ SUMMARY FOR {feature_set_name.upper()}:\")\n",
    "        for model_name, result in feature_results.items():\n",
    "            print(f\"   {model_name:20}: Accuracy={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ‰ SAFE TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ],
   "id": "9270a7537e9da594",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T12:29:32.627716Z",
     "start_time": "2025-08-04T12:29:13.801714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_safe_pipeline():\n",
    "    \"\"\"Execute safe Phase 2 pipeline with data validation\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸš€ STARTING SAFE PHASE 2 PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Step 1: Update VLE features with safe calculations\n",
    "        print(\"\\n1ï¸âƒ£ RECREATING VLE FEATURES SAFELY...\")\n",
    "        vle_features_safe = create_vle_features_safe(oulad_data_clean['student_vle'])\n",
    "\n",
    "        # Step 2: Recreate combined dataset with safe features\n",
    "        print(\"\\n2ï¸âƒ£ RECREATING COMBINED DATASET...\")\n",
    "        combined_safe = oulad_data_clean['student_info'].copy()\n",
    "\n",
    "        # Add registration data\n",
    "        registration = oulad_data_clean['student_registration'][\n",
    "            ['code_module', 'code_presentation', 'id_student', 'completed_course', 'withdrawal_status']\n",
    "        ]\n",
    "        combined_safe = combined_safe.merge(registration, on=['code_module', 'code_presentation', 'id_student'], how='inner')\n",
    "\n",
    "        # Add safe VLE features\n",
    "        combined_safe = combined_safe.merge(vle_features_safe, on='id_student', how='left')\n",
    "\n",
    "        # Add assessment features\n",
    "        assessment_features = create_assessment_features(oulad_data_clean['student_assessment'])\n",
    "        combined_safe = combined_safe.merge(assessment_features, on='id_student', how='left')\n",
    "\n",
    "        print(f\"âœ… Safe combined dataset: {combined_safe.shape}\")\n",
    "\n",
    "        # Step 3: Prepare modeling data\n",
    "        print(\"\\n3ï¸âƒ£ PREPARING SAFE MODELING DATA...\")\n",
    "        processed_data_safe, all_features, behavior_only, target_encoder, le_dict = prepare_modeling_data(combined_safe)\n",
    "\n",
    "        print(f\"âœ… Safe processed data: {processed_data_safe.shape}\")\n",
    "\n",
    "        # Step 4: Train models safely\n",
    "        print(\"\\n4ï¸âƒ£ TRAINING MODELS SAFELY...\")\n",
    "        feature_sets = {\n",
    "            'all_features': all_features,\n",
    "            'behavior_only': behavior_only\n",
    "        }\n",
    "\n",
    "        model_results = train_baseline_models_safe(processed_data_safe, feature_sets)\n",
    "\n",
    "        return model_results, processed_data_safe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Safe pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# RUN THE SAFE PIPELINE\n",
    "print(\"EXECUTING SAFE PIPELINE...\")\n",
    "model_results, processed_data_safe = run_safe_pipeline()\n"
   ],
   "id": "5c488595f10fef5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING SAFE PIPELINE...\n",
      "ğŸš€ STARTING SAFE PHASE 2 PIPELINE\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ RECREATING VLE FEATURES SAFELY...\n",
      "ğŸ”„ Creating VLE features with safe calculations...\n",
      "âœ… VLE features created for 26074 students\n",
      "\n",
      "2ï¸âƒ£ RECREATING COMBINED DATASET...\n",
      "âœ… Safe combined dataset: (32593, 29)\n",
      "\n",
      "3ï¸âƒ£ PREPARING SAFE MODELING DATA...\n",
      "âœ… Safe processed data: (32593, 30)\n",
      "\n",
      "4ï¸âƒ£ TRAINING MODELS SAFELY...\n",
      "ğŸš€ STARTING SAFE BASELINE MODEL TRAINING\n",
      "Dataset shape: (32593, 30)\n",
      "Target distribution:\n",
      "target\n",
      "2    12361\n",
      "3    10156\n",
      "1     7052\n",
      "0     3024\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ TRAINING ON ALL_FEATURES FEATURE SET\n",
      "============================================================\n",
      "âœ… Initial data: X=(32593, 15), y=(32593,)\n",
      "ğŸ§¹ CLEANING INFINITE AND EXTREME VALUES\n",
      "   âœ… Filled total_clicks missing values with median: 903.00\n",
      "   âœ… Filled avg_clicks_per_session missing values with median: 3.13\n",
      "   âœ… Filled active_days missing values with median: 55.00\n",
      "   âœ… Filled daily_engagement_rate missing values with median: 0.30\n",
      "   âœ… Filled avg_assessment_score missing values with median: 75.71\n",
      "   âœ… Filled total_assessments missing values with median: 7.00\n",
      "   âœ… Filled total_sessions missing values with median: 291.00\n",
      "   âœ… Filled engagement_duration missing values with median: 238.00\n",
      "âœ… Data cleaning complete. Issues found in: []\n",
      "âœ… Data validated: No infinite values, No missing values\n",
      "âœ… Train/Test split: Train=(26074, 15), Test=(6519, 15)\n",
      "âœ… Feature scaling successful\n",
      "\n",
      "ğŸ”„ Training Random Forest...\n",
      "âœ… Random Forest Results:\n",
      "   ğŸ“Š Accuracy: 0.8412\n",
      "   ğŸ“Š F1-Score (weighted): 0.8361\n",
      "\n",
      "ğŸ”„ Training Logistic Regression...\n",
      "âœ… Logistic Regression Results:\n",
      "   ğŸ“Š Accuracy: 0.8382\n",
      "   ğŸ“Š F1-Score (weighted): 0.8321\n",
      "\n",
      "ğŸ”„ Training XGBoost...\n",
      "âœ… XGBoost Results:\n",
      "   ğŸ“Š Accuracy: 0.8500\n",
      "   ğŸ“Š F1-Score (weighted): 0.8461\n",
      "\n",
      "ğŸ“ˆ SUMMARY FOR ALL_FEATURES:\n",
      "   Random Forest       : Accuracy=0.8412, F1=0.8361\n",
      "   Logistic Regression : Accuracy=0.8382, F1=0.8321\n",
      "   XGBoost             : Accuracy=0.8500, F1=0.8461\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ TRAINING ON BEHAVIOR_ONLY FEATURE SET\n",
      "============================================================\n",
      "âœ… Initial data: X=(32593, 12), y=(32593,)\n",
      "ğŸ§¹ CLEANING INFINITE AND EXTREME VALUES\n",
      "   âœ… Filled total_clicks missing values with median: 903.00\n",
      "   âœ… Filled avg_clicks_per_session missing values with median: 3.13\n",
      "   âœ… Filled active_days missing values with median: 55.00\n",
      "   âœ… Filled daily_engagement_rate missing values with median: 0.30\n",
      "   âœ… Filled total_sessions missing values with median: 291.00\n",
      "   âœ… Filled engagement_duration missing values with median: 238.00\n",
      "âœ… Data cleaning complete. Issues found in: []\n",
      "âœ… Data validated: No infinite values, No missing values\n",
      "âœ… Train/Test split: Train=(26074, 12), Test=(6519, 12)\n",
      "âœ… Feature scaling successful\n",
      "\n",
      "ğŸ”„ Training Random Forest...\n",
      "âœ… Random Forest Results:\n",
      "   ğŸ“Š Accuracy: 0.8075\n",
      "   ğŸ“Š F1-Score (weighted): 0.7827\n",
      "\n",
      "ğŸ”„ Training Logistic Regression...\n",
      "âœ… Logistic Regression Results:\n",
      "   ğŸ“Š Accuracy: 0.8113\n",
      "   ğŸ“Š F1-Score (weighted): 0.7752\n",
      "\n",
      "ğŸ”„ Training XGBoost...\n",
      "âœ… XGBoost Results:\n",
      "   ğŸ“Š Accuracy: 0.8127\n",
      "   ğŸ“Š F1-Score (weighted): 0.7885\n",
      "\n",
      "ğŸ“ˆ SUMMARY FOR BEHAVIOR_ONLY:\n",
      "   Random Forest       : Accuracy=0.8075, F1=0.7827\n",
      "   Logistic Regression : Accuracy=0.8113, F1=0.7752\n",
      "   XGBoost             : Accuracy=0.8127, F1=0.7885\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ SAFE TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
