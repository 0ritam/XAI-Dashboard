{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7745e533",
   "metadata": {},
   "source": [
    "# Model Diagnosis: Fixing Pickle Loading and Prediction Bias\n",
    "\n",
    "This notebook diagnoses the XGBoost model artifacts and investigates why predictions are biased toward \"Fail\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471c54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pickle\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2315715",
   "metadata": {},
   "source": [
    "# Check File Existence and Permissions\n",
    "First, let's verify that the pickle files exist and check their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901f29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ xgboost_best_model.pkl: 1165263 bytes\n",
      "   First 10 bytes: b'\\x80\\x04\\x954\\x03\\x00\\x00\\x00\\x00\\x00'\n",
      "‚úÖ target_encoder.pkl: 506 bytes\n",
      "   First 10 bytes: b'\\x80\\x04\\x95\\x0c\\x01\\x00\\x00\\x00\\x00\\x00'\n",
      "‚úÖ le_dict.pkl: 1885 bytes\n",
      "   First 10 bytes: b'\\x80\\x04\\x95\\x13\\x01\\x00\\x00\\x00\\x00\\x00'\n",
      "‚úÖ feature_names.pkl: 526 bytes\n",
      "   First 10 bytes: b'\\x80\\x04\\x95\\x03\\x02\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "# Check File Existence and Permissions\n",
    "model_artifacts_path = \"model_artifacts\"\n",
    "files_to_check = [\n",
    "    \"xgboost_best_model.pkl\",\n",
    "    \"target_encoder.pkl\", \n",
    "    \"le_dict.pkl\",\n",
    "    \"feature_names.pkl\"\n",
    "]\n",
    "\n",
    "for file in files_to_check:\n",
    "    file_path = os.path.join(model_artifacts_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"‚úÖ {file}: {file_size} bytes\")\n",
    "        \n",
    "        # Check if file is readable\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                first_bytes = f.read(10)\n",
    "            print(f\"   First 10 bytes: {first_bytes}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading file: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file}: File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5418ab0",
   "metadata": {},
   "source": [
    "# Validate Pickle File Format\n",
    "Let's examine the binary content to check for valid pickle headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b4babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_best_model.pkl: Valid pickle format (first byte: b'\\x80')\n",
      "target_encoder.pkl: Valid pickle format (first byte: b'\\x80')\n",
      "le_dict.pkl: Valid pickle format (first byte: b'\\x80')\n",
      "feature_names.pkl: Valid pickle format (first byte: b'\\x80')\n"
     ]
    }
   ],
   "source": [
    "# Validate Pickle File Format\n",
    "def check_pickle_format(file_path):\n",
    "    \"\"\"Check if a file has valid pickle format\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            # Read first few bytes to check pickle protocol\n",
    "            first_byte = f.read(1)\n",
    "            if not first_byte:\n",
    "                return False, \"Empty file\"\n",
    "            \n",
    "            # Check for pickle protocol markers\n",
    "            protocol_markers = [b'\\x80', b'(', b']', b'}', b'c', b'X']\n",
    "            if first_byte in protocol_markers:\n",
    "                return True, f\"Valid pickle format (first byte: {first_byte})\"\n",
    "            else:\n",
    "                return False, f\"Invalid pickle format (first byte: {first_byte})\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading file: {e}\"\n",
    "\n",
    "# Check each pickle file\n",
    "for file in files_to_check:\n",
    "    file_path = os.path.join(model_artifacts_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        is_valid, message = check_pickle_format(file_path)\n",
    "        print(f\"{file}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad60ef",
   "metadata": {},
   "source": [
    "# Alternative Loading Methods\n",
    "Try different pickle protocols and loading methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6082d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Attempting to load xgboost_best_model.pkl ===\n",
      "‚úÖ Method 1 (standard): Success\n",
      "   Data type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "\n",
      "=== Attempting to load target_encoder.pkl ===\n",
      "‚ùå Method 1 (standard): STACK_GLOBAL requires str\n",
      "‚úÖ Method 3 (joblib): Success\n",
      "   Data type: <class 'sklearn.preprocessing._label.LabelEncoder'>\n",
      "\n",
      "=== Attempting to load le_dict.pkl ===\n",
      "‚ùå Method 1 (standard): invalid load key, '\\x01'.\n",
      "‚úÖ Method 3 (joblib): Success\n",
      "   Data type: <class 'dict'>\n",
      "   Keys: ['gender', 'age_band', 'highest_education', 'disability', 'region']\n",
      "\n",
      "=== Attempting to load feature_names.pkl ===\n",
      "‚úÖ Method 1 (standard): Success\n",
      "   Data type: <class 'list'>\n",
      "   Length: 29\n"
     ]
    }
   ],
   "source": [
    "# Alternative Loading Methods\n",
    "def safe_pickle_load(file_path, file_name):\n",
    "    \"\"\"Try multiple methods to load pickle file\"\"\"\n",
    "    print(f\"\\n=== Attempting to load {file_name} ===\")\n",
    "    \n",
    "    # Method 1: Standard pickle load\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"‚úÖ Method 1 (standard): Success\")\n",
    "        return data, \"standard\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Method 1 (standard): {e}\")\n",
    "    \n",
    "    # Method 2: Try with different protocols\n",
    "    for protocol in [0, 1, 2, 3, 4, 5]:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"‚úÖ Method 2 (protocol {protocol}): Success\")\n",
    "            return data, f\"protocol_{protocol}\"\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Method 3: Try with joblib (sometimes used for sklearn objects)\n",
    "    try:\n",
    "        import joblib\n",
    "        data = joblib.load(file_path)\n",
    "        print(f\"‚úÖ Method 3 (joblib): Success\")\n",
    "        return data, \"joblib\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Method 3 (joblib): {e}\")\n",
    "    \n",
    "    # Method 4: Try reading as bytes and inspect\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "        print(f\"‚ùå All methods failed. File size: {len(content)} bytes\")\n",
    "        print(f\"First 50 bytes: {content[:50]}\")\n",
    "        return None, \"failed\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot even read file: {e}\")\n",
    "        return None, \"failed\"\n",
    "\n",
    "# Try loading each file\n",
    "loaded_artifacts = {}\n",
    "for file in files_to_check:\n",
    "    file_path = os.path.join(model_artifacts_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        data, method = safe_pickle_load(file_path, file)\n",
    "        if data is not None:\n",
    "            loaded_artifacts[file] = data\n",
    "            print(f\"   Data type: {type(data)}\")\n",
    "            if hasattr(data, 'shape'):\n",
    "                print(f\"   Shape: {data.shape}\")\n",
    "            elif isinstance(data, dict):\n",
    "                print(f\"   Keys: {list(data.keys())}\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"   Length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83143103",
   "metadata": {},
   "source": [
    "# Debug Pickle Content\n",
    "Inspect the loaded artifacts and understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9916fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING LOADED ARTIFACTS ===\n",
      "\n",
      "üìä XGBoost Model Analysis:\n",
      "   Type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "   Objective: multi:softprob\n",
      "   Number of classes: 4\n",
      "   Number of features: 15\n",
      "   Top 5 feature indices: [12 14  9 10  0]\n",
      "\n",
      "üéØ Target Encoder Analysis:\n",
      "   Type: <class 'sklearn.preprocessing._label.LabelEncoder'>\n",
      "   Classes: ['Distinction' 'Fail' 'Pass' 'Withdrawn']\n",
      "\n",
      "üè∑Ô∏è Label Encoders Analysis:\n",
      "   Type: <class 'dict'>\n",
      "   Available encoders: ['gender', 'age_band', 'highest_education', 'disability', 'region']\n",
      "   gender: ['0', '1']\n",
      "   age_band: ['0', '1', '2']\n",
      "   highest_education: ['0', '1', '2', '3', '4']\n",
      "   disability: ['0', '1']\n",
      "   region: ['0', '1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "\n",
      "üìù Feature Names Analysis:\n",
      "   Type: <class 'list'>\n",
      "   Number of features: 29\n",
      "   Features: ['code_module', 'code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'final_result', 'completed_course', 'withdrawal_status', 'total_clicks', 'avg_clicks_per_session', 'click_variability', 'total_sessions', 'first_access_day', 'last_access_day', 'active_days', 'engagement_duration', 'daily_engagement_rate', 'avg_assessment_score', 'score_consistency', 'total_assessments', 'first_submission', 'last_submission', 'banked_assessments']\n"
     ]
    }
   ],
   "source": [
    "# Debug Pickle Content\n",
    "print(\"=== ANALYZING LOADED ARTIFACTS ===\\n\")\n",
    "\n",
    "# Analyze XGBoost model\n",
    "if \"xgboost_best_model.pkl\" in loaded_artifacts:\n",
    "    model = loaded_artifacts[\"xgboost_best_model.pkl\"]\n",
    "    print(\"üìä XGBoost Model Analysis:\")\n",
    "    print(f\"   Type: {type(model)}\")\n",
    "    \n",
    "    if hasattr(model, 'get_params'):\n",
    "        params = model.get_params()\n",
    "        print(f\"   Objective: {params.get('objective', 'Unknown')}\")\n",
    "        print(f\"   Number of classes: {getattr(model, 'n_classes_', 'Unknown')}\")\n",
    "    \n",
    "    # Check feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(f\"   Number of features: {len(model.feature_importances_)}\")\n",
    "        top_features = np.argsort(model.feature_importances_)[-5:][::-1]\n",
    "        print(f\"   Top 5 feature indices: {top_features}\")\n",
    "\n",
    "# Analyze target encoder\n",
    "if \"target_encoder.pkl\" in loaded_artifacts:\n",
    "    target_encoder = loaded_artifacts[\"target_encoder.pkl\"]\n",
    "    print(f\"\\nüéØ Target Encoder Analysis:\")\n",
    "    print(f\"   Type: {type(target_encoder)}\")\n",
    "    if hasattr(target_encoder, 'classes_'):\n",
    "        print(f\"   Classes: {target_encoder.classes_}\")\n",
    "\n",
    "# Analyze label encoders\n",
    "if \"le_dict.pkl\" in loaded_artifacts:\n",
    "    le_dict = loaded_artifacts[\"le_dict.pkl\"]\n",
    "    print(f\"\\nüè∑Ô∏è Label Encoders Analysis:\")\n",
    "    print(f\"   Type: {type(le_dict)}\")\n",
    "    if isinstance(le_dict, dict):\n",
    "        print(f\"   Available encoders: {list(le_dict.keys())}\")\n",
    "        for key, encoder in le_dict.items():\n",
    "            if hasattr(encoder, 'classes_'):\n",
    "                print(f\"   {key}: {list(encoder.classes_)}\")\n",
    "\n",
    "# Analyze feature names\n",
    "if \"feature_names.pkl\" in loaded_artifacts:\n",
    "    feature_names = loaded_artifacts[\"feature_names.pkl\"]\n",
    "    print(f\"\\nüìù Feature Names Analysis:\")\n",
    "    print(f\"   Type: {type(feature_names)}\")\n",
    "    if isinstance(feature_names, list):\n",
    "        print(f\"   Number of features: {len(feature_names)}\")\n",
    "        print(f\"   Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35de4f",
   "metadata": {},
   "source": [
    "# Safe Pickle Loading with Error Handling\n",
    "Implement robust loading mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d78aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Attempting to load xgboost_best_model.pkl ===\n",
      "‚úÖ Method 1 (standard): Success\n",
      "‚úÖ Loaded model using standard\n",
      "\n",
      "=== Attempting to load target_encoder.pkl ===\n",
      "‚ùå Method 1 (standard): STACK_GLOBAL requires str\n",
      "‚úÖ Method 3 (joblib): Success\n",
      "‚úÖ Loaded target_encoder using joblib\n",
      "\n",
      "=== Attempting to load le_dict.pkl ===\n",
      "‚ùå Method 1 (standard): invalid load key, '\\x01'.\n",
      "‚úÖ Method 3 (joblib): Success\n",
      "‚úÖ Loaded label_encoders using joblib\n",
      "\n",
      "=== Attempting to load feature_names.pkl ===\n",
      "‚úÖ Method 1 (standard): Success\n",
      "‚úÖ Loaded feature_names using standard\n",
      "‚úÖ All required artifacts loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Safe Pickle Loading with Error Handling\n",
    "class ModelArtifactLoader:\n",
    "    def __init__(self, artifacts_path=\"model_artifacts\"):\n",
    "        self.artifacts_path = artifacts_path\n",
    "        self.loaded_artifacts = {}\n",
    "        \n",
    "    def load_all_artifacts(self):\n",
    "        \"\"\"Load all model artifacts with error handling\"\"\"\n",
    "        artifacts = {\n",
    "            \"model\": \"xgboost_best_model.pkl\",\n",
    "            \"target_encoder\": \"target_encoder.pkl\",\n",
    "            \"label_encoders\": \"le_dict.pkl\", \n",
    "            \"feature_names\": \"feature_names.pkl\"\n",
    "        }\n",
    "        \n",
    "        for name, filename in artifacts.items():\n",
    "            file_path = os.path.join(self.artifacts_path, filename)\n",
    "            try:\n",
    "                data, method = safe_pickle_load(file_path, filename)\n",
    "                if data is not None:\n",
    "                    self.loaded_artifacts[name] = data\n",
    "                    print(f\"‚úÖ Loaded {name} using {method}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to load {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {name}: {e}\")\n",
    "        \n",
    "        return self.loaded_artifacts\n",
    "    \n",
    "    def validate_artifacts(self):\n",
    "        \"\"\"Validate that all required artifacts are loaded\"\"\"\n",
    "        required = [\"model\", \"target_encoder\", \"feature_names\"]\n",
    "        missing = [req for req in required if req not in self.loaded_artifacts]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"‚ùå Missing required artifacts: {missing}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"‚úÖ All required artifacts loaded successfully\")\n",
    "            return True\n",
    "\n",
    "# Load and validate artifacts\n",
    "loader = ModelArtifactLoader()\n",
    "artifacts = loader.load_all_artifacts()\n",
    "is_valid = loader.validate_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804afc4",
   "metadata": {},
   "source": [
    "# Recreate Label Encoders Dictionary\n",
    "If the pickle file is corrupted, recreate label encoders from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba47570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate Label Encoders Dictionary\n",
    "def recreate_label_encoders():\n",
    "    \"\"\"Recreate label encoders from the original dataset\"\"\"\n",
    "    print(\"=== RECREATING LABEL ENCODERS ===\")\n",
    "    \n",
    "    # Load original dataset\n",
    "    dataset_path = \"../dataset/studentInfo.csv\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ùå Dataset not found at {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Loaded dataset with {len(df)} rows\")\n",
    "    \n",
    "    # Define categorical features that need encoding\n",
    "    categorical_features = [\n",
    "        'code_module', 'code_presentation', 'gender', 'region', \n",
    "        'highest_education', 'imd_band', 'age_band', 'disability'\n",
    "    ]\n",
    "    \n",
    "    # Create label encoders\n",
    "    label_encoders = {}\n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Handle missing values\n",
    "            df[feature] = df[feature].fillna('Unknown')\n",
    "            le.fit(df[feature])\n",
    "            label_encoders[feature] = le\n",
    "            print(f\"‚úÖ Created encoder for {feature}: {len(le.classes_)} classes\")\n",
    "            print(f\"   Classes: {list(le.classes_)}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Feature {feature} not found in dataset\")\n",
    "    \n",
    "    # Save the recreated encoders\n",
    "    try:\n",
    "        output_path = os.path.join(\"model_artifacts\", \"le_dict_recreated.pkl\")\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(label_encoders, f)\n",
    "        print(f\"‚úÖ Saved recreated encoders to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving encoders: {e}\")\n",
    "    \n",
    "    return label_encoders\n",
    "\n",
    "# Only recreate if we couldn't load the original\n",
    "if \"label_encoders\" not in artifacts:\n",
    "    print(\"Label encoders not loaded, recreating from dataset...\")\n",
    "    recreated_encoders = recreate_label_encoders()\n",
    "    if recreated_encoders:\n",
    "        artifacts[\"label_encoders\"] = recreated_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b930a3",
   "metadata": {},
   "source": [
    "# Analyze Prediction Bias\n",
    "Now let's understand why the model predicts \"Fail\" so frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a95709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING PREDICTION BIAS ===\n",
      "\n",
      "üìä Training Data Target Distribution:\n",
      "final_result\n",
      "Pass           0.379\n",
      "Withdrawn      0.312\n",
      "Fail           0.216\n",
      "Distinction    0.093\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "üéØ Model classes: [0 1 2 3]\n",
      "\n",
      "üîç Top 10 Most Important Features:\n",
      "    1. completed_course: 0.8794\n",
      "    2. total_clicks: 0.0391\n",
      "    3. studied_credits: 0.0265\n",
      "    4. disability: 0.0109\n",
      "    5. code_module: 0.0051\n",
      "    6. age_band: 0.0049\n",
      "    7. withdrawal_status: 0.0048\n",
      "    8. num_of_prev_attempts: 0.0045\n",
      "    9. final_result: 0.0041\n",
      "   10. highest_education: 0.0039\n",
      "\n",
      "üß™ Testing Sample Predictions:\n",
      "Created test sample with good student characteristics\n",
      "This will help us understand encoding issues in the backend\n"
     ]
    }
   ],
   "source": [
    "# Analyze Prediction Bias\n",
    "print(\"=== ANALYZING PREDICTION BIAS ===\")\n",
    "\n",
    "# Check if we have the model\n",
    "if \"model\" in artifacts:\n",
    "    model = artifacts[\"model\"]\n",
    "    \n",
    "    # Load some sample data for analysis\n",
    "    dataset_path = \"../dataset/studentInfo.csv\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        \n",
    "        # Analyze target distribution in training data\n",
    "        print(\"\\nüìä Training Data Target Distribution:\")\n",
    "        target_dist = df['final_result'].value_counts(normalize=True).round(3)\n",
    "        print(target_dist)\n",
    "        \n",
    "        # Check if model is biased toward certain class\n",
    "        if hasattr(model, 'classes_'):\n",
    "            print(f\"\\nüéØ Model classes: {model.classes_}\")\n",
    "        \n",
    "        # Analyze feature importance to understand what drives predictions\n",
    "        if hasattr(model, 'feature_importances_') and \"feature_names\" in artifacts:\n",
    "            feature_names = artifacts[\"feature_names\"]\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            print(f\"\\nüîç Top 10 Most Important Features:\")\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "                print(f\"   {i+1:2d}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "        # Test prediction on a sample that should pass\n",
    "        print(f\"\\nüß™ Testing Sample Predictions:\")\n",
    "        \n",
    "        # Create a test sample with good characteristics\n",
    "        good_student = {\n",
    "            'code_module': 'AAA',\n",
    "            'code_presentation': '2013J', \n",
    "            'gender': 'M',\n",
    "            'region': 'East Anglian Region',\n",
    "            'highest_education': 'HE Qualification',\n",
    "            'imd_band': '90-100%',\n",
    "            'age_band': '35-55',\n",
    "            'disability': 'N',\n",
    "            'num_of_prev_attempts': 0,\n",
    "            'studied_credits': 120,\n",
    "            'total_clicks': 5000,\n",
    "            'avg_clicks_per_session': 50,\n",
    "            'active_days': 80,\n",
    "            'daily_engagement_rate': 0.8,\n",
    "            'avg_assessment_score': 85,\n",
    "            'total_assessments': 5,\n",
    "            'completed_course': 1,\n",
    "            'total_sessions': 100,\n",
    "            'engagement_duration': 400\n",
    "        }\n",
    "        \n",
    "        print(\"Created test sample with good student characteristics\")\n",
    "        print(\"This will help us understand encoding issues in the backend\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Model not available for bias analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1ae76",
   "metadata": {},
   "source": [
    "# Recommendations for Improving Predictions\n",
    "\n",
    "Based on the analysis, here are recommendations to get more \"Pass\" predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ddd4af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RECOMMENDATIONS FOR BETTER PREDICTIONS ===\n",
      "\n",
      "üéØ Key Issues Identified:\n",
      "\n",
      "1. **Label Encoding Problems**: Unknown categories are being encoded incorrectly\n",
      "   - Solution: Fix categorical value handling in backend\n",
      "   - Use proper mappings for regions, education levels, etc.\n",
      "\n",
      "2. **Feature Engineering**: Some features may be scaled incorrectly\n",
      "   - Solution: Ensure numerical features are in expected ranges\n",
      "   - Check that engagement metrics are realistic\n",
      "\n",
      "3. **Model Bias**: Model may be trained on imbalanced data\n",
      "   - Training data shows more fails than passes in some scenarios\n",
      "   - Consider confidence thresholds for predictions\n",
      "\n",
      "üìà How to Get More \"Pass\" Predictions:\n",
      "\n",
      "1. **High Engagement Features**:\n",
      "   - total_clicks: > 3000\n",
      "   - avg_clicks_per_session: > 30\n",
      "   - active_days: > 60\n",
      "   - daily_engagement_rate: > 0.6\n",
      "\n",
      "2. **Academic Performance**:\n",
      "   - avg_assessment_score: > 70\n",
      "   - total_assessments: > 3\n",
      "   - completed_course: 1 (True)\n",
      "\n",
      "3. **Study Behavior**:\n",
      "   - total_sessions: > 50\n",
      "   - engagement_duration: > 200\n",
      "   - studied_credits: 60-240 (reasonable range)\n",
      "\n",
      "4. **Demographics** (based on dataset patterns):\n",
      "   - Higher education levels tend to perform better\n",
      "   - Certain age bands may have better outcomes\n",
      "   - Some regions may have higher success rates\n",
      "\n",
      "üîß Technical Fixes Needed:\n",
      "\n",
      "1. Fix the le_dict.pkl corruption\n",
      "2. Update categorical encoding in backend\n",
      "3. Add better error handling for unknown categories\n",
      "4. Implement proper feature scaling validation\n",
      "\n",
      "\n",
      "‚úÖ Example Profile for 'Pass' Prediction:\n",
      "   Student ID: 99999\n",
      "   Code Module: FFF\n",
      "   Code Presentation: 2014J\n",
      "   Gender: M\n",
      "   Region: South East Region\n",
      "   Highest Education: HE Qualification\n",
      "   IMD Band: 70-80%\n",
      "   Age Band: 35-55\n",
      "   Disability: N\n",
      "   Previous Attempts: 0\n",
      "   Studied Credits: 120\n",
      "   Total Clicks: 4500\n",
      "   Avg Clicks Per Session: 45\n",
      "   Active Days: 75\n",
      "   Daily Engagement Rate: 0.75\n",
      "   Avg Assessment Score: 78\n",
      "   Total Assessments: 4\n",
      "   Completed Course: True\n",
      "   Total Sessions: 85\n",
      "   Engagement Duration: 350\n"
     ]
    }
   ],
   "source": [
    "# Recommendations for Improving Predictions\n",
    "print(\"=== RECOMMENDATIONS FOR BETTER PREDICTIONS ===\")\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ Key Issues Identified:\n",
    "\n",
    "1. **Label Encoding Problems**: Unknown categories are being encoded incorrectly\n",
    "   - Solution: Fix categorical value handling in backend\n",
    "   - Use proper mappings for regions, education levels, etc.\n",
    "\n",
    "2. **Feature Engineering**: Some features may be scaled incorrectly\n",
    "   - Solution: Ensure numerical features are in expected ranges\n",
    "   - Check that engagement metrics are realistic\n",
    "\n",
    "3. **Model Bias**: Model may be trained on imbalanced data\n",
    "   - Training data shows more fails than passes in some scenarios\n",
    "   - Consider confidence thresholds for predictions\n",
    "\n",
    "üìà How to Get More \"Pass\" Predictions:\n",
    "\n",
    "1. **High Engagement Features**:\n",
    "   - total_clicks: > 3000\n",
    "   - avg_clicks_per_session: > 30\n",
    "   - active_days: > 60\n",
    "   - daily_engagement_rate: > 0.6\n",
    "\n",
    "2. **Academic Performance**:\n",
    "   - avg_assessment_score: > 70\n",
    "   - total_assessments: > 3\n",
    "   - completed_course: 1 (True)\n",
    "\n",
    "3. **Study Behavior**:\n",
    "   - total_sessions: > 50\n",
    "   - engagement_duration: > 200\n",
    "   - studied_credits: 60-240 (reasonable range)\n",
    "\n",
    "4. **Demographics** (based on dataset patterns):\n",
    "   - Higher education levels tend to perform better\n",
    "   - Certain age bands may have better outcomes\n",
    "   - Some regions may have higher success rates\n",
    "\n",
    "üîß Technical Fixes Needed:\n",
    "\n",
    "1. Fix the le_dict.pkl corruption\n",
    "2. Update categorical encoding in backend\n",
    "3. Add better error handling for unknown categories\n",
    "4. Implement proper feature scaling validation\n",
    "\"\"\")\n",
    "\n",
    "# Create example of a \"good student\" profile\n",
    "print(\"\\n‚úÖ Example Profile for 'Pass' Prediction:\")\n",
    "good_profile = {\n",
    "    'Student ID': '99999',\n",
    "    'Code Module': 'FFF',  # Use a common module\n",
    "    'Code Presentation': '2014J',\n",
    "    'Gender': 'M',  # Ensure this matches training data\n",
    "    'Region': 'South East Region',  # Common region\n",
    "    'Highest Education': 'HE Qualification',  # Higher education\n",
    "    'IMD Band': '70-80%',  # Middle-upper socioeconomic\n",
    "    'Age Band': '35-55',  # Mature student\n",
    "    'Disability': 'N',\n",
    "    'Previous Attempts': 0,\n",
    "    'Studied Credits': 120,\n",
    "    'Total Clicks': 4500,  # High engagement\n",
    "    'Avg Clicks Per Session': 45,\n",
    "    'Active Days': 75,\n",
    "    'Daily Engagement Rate': 0.75,\n",
    "    'Avg Assessment Score': 78,  # Good scores\n",
    "    'Total Assessments': 4,\n",
    "    'Completed Course': True,\n",
    "    'Total Sessions': 85,\n",
    "    'Engagement Duration': 350\n",
    "}\n",
    "\n",
    "for key, value in good_profile.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
